Asymptotic Complexity::
	O is worst case
	Ω (omega) is best case
	θ(theta) occurs when O worst case and Ω best case are the same

Common complexities::
	O(1) constant time; does not increase as input size increases; ex: getting the first element of an array
	O(n) linear time; increases with input size; ex: linear search through an array
	O(n^2) ex: insertion sort
	O(log n) ex: binary search
	O(n log n) ex: quicksort at average case
		n log n is the same as saying (log n)^n

Formal Definition::
		sometimes f(n) will appear as T(n) for Time
	1) A function f(n) is O(g(n)) if a positive constant c is of a certain value and a positive number n0 (n not) is of a certain value
		>>>in order for this to be true, f(n) must be LESS than or equal to c times g(n) for all n < n0
		>>>I choose g(n). I choose a constant c. I use these to find n0 where the statement is true starting at 0
	EX: f(n) = 4n^2 + 16n + 2
		is f(n) => O(n^4)?
		I choose constant c to be 1
		| n | 4n^2 + 16n + 2 < 1 * n^4 |
		| 0 | 2 < 0 NO
		| 1 | 22 < 1 NO
		| ..|
		| 4 | 130 < 256 YES when n0 is 4
		*SHORTCUT: take the dominant term and compare with term c * g(n); drop the constants in front of both terms
			>>>drop all terms in 4n^2 + 16n + 2 except for 4n^2 which is just n^2; take c * n^4 which is just n^4
			>>>can easily see that n^2 <= n^4 which satisfies f(n) <= c * g(n); the answer is YES
			EXTRA INFO: Why do only consider the dominant term? Because this takes up more time in a function as the input increases;
				the rest of the time is fairly insignificant

	2) A function f(n) is Ω(g(n)) if a positive constant c is of a certain value and a positive number n0 (n not) is of a certain value
		>>>in order for this to be true, f(n) must be GREATER than or equal to c times g(n) for all n < n0

	3) A function f(n) is θ(g(n)) iff f(n) is O(g(n)) AND f(n) is Ω(g(n))